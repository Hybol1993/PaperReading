An Analysis of Linux Scalability to Many Cores 阅读报告
============

<center>毛海宇 2015310607 </center>
-----------------------------------------------------

##背景##
####why need to do####
很多人怀疑传统的Linux系统架构不能支持可扩展性。

####how previous research do####
通过使用粗粒度的锁等等技术。

####the main contribution of this paper####
>1. 一组名为MOSBENCH的benchmark，用来评测操作系统的可扩展性
>2. 16个对Linux 2.6.35-rc5 kernel的改善，命名为PK(patched kernel)
>3. 描述改善MOSBENCH中应用的可扩展性的必要技术

最终的结论：没有直接的因为可扩展性而放弃传统内核设计的理由

##MOSBENCH 测试应用##

为了强调内核，本文选择两类的应用：
>1. 之前的工作都显示其在Linux上**没有很好的扩展性**的应用
>2. 为**并行和内核集中**而设计的应用

本文使用内存里的tmpfs文件系统来模拟**无磁盘**的情形，以避免大多数应用因为磁盘写而产生的瓶颈。这些应用都起到了强调内核中那些在单核上运行花掉大多CPU时间的重要部分：

####1. Mail server####
Exim是一个mail server：用一个服务器来处理listens，并且为每个链接生成一个新进程，用来准备接受要发来的邮件，并且对他们做排序，归类，删除过期邮件，把信息记录到共享的日志文件中。每个链接进程会被唤醒两次来接受信息。在多用户高并发的情况下，Exim体现出高并行性，在单核上用了69%的内核时间，**强调了进程创建以及小文件的创建和删除**。

####2. Object cache####
memcached是在内存中的k-v对存储常用来提高网页应用性能的。当一个memcached的服务器跑在多核上时，保护k-v哈希表的内部锁会成为它的瓶颈。为了避免，跑多个mamcached服务器，每个都有自己的端口，直接从服务器中查找，让服务器们能并行处理请求。当请求小时，用80%的内核时间在单核上处理包，**它着重强调网络栈**。

####3. Web server ####
将Apache服务器配置成一个实例在80端口监听，每个实例上有一个进程在单核上跑，每个进程有一个线程池来提供通信服务，一个线程用来接受将要到来的信息，其他的则处理这些信息。除了网络栈，由于它每来一个请求都会打开一个文件，这个配置**着重于文件系统，尤其是目录名的查找**。在单核上，其用掉60%的内核执行时间。

####4. Database ####
PostgreSQL不像本文其他负载，它能更大限度发挥内部共享数据结构和同步性的作用。他也**强调内核中很多共享资源**。它依赖于快照隔离（一种避免大多数读锁的并发控制方法）。其实在共享模式下，它中间的很多写操作只需要粗粒度的锁。所以在实际中，它**不仅因为内核而产生瓶颈，也因为自身的代码**。在只读负载下，它避免了大多数的瓶颈，单核时仅使用内核时间的1.5%，但是48核时激涨到82%。

####5. Parallel build ####
通过用gmake编译Linux 2.6.35-rc5 kernel来作为本文的benchmark。gmake会创建多于核数的进程，并且读写很多文件。占用76%系统时间。

####6. File indexer ####
pedsort是用来查询网页的。它首先从作业队列中取出输入文件，读每个文件并且对每个词译码。这一阶段是计算敏感的和文件系统敏感的。为了避免，把队列中的文件排好序。它在单核上运行时，仅使用1.9%的内核时间，在48核上上升到23%。

####7. MapReduce ####
Metis是多核单服务器MapRedue文件库。本文用它来生成反索引。这个负载分配大量的内存来记录临时表，**强调内核内存分配器和页错误处理**。单核时间3%，48核16%。

##内核优化##
根据阿姆达定律，串行部分会阻碍随核数增加而性能增加。列举常见的串行并相关的类型：
>1. 任务锁住一个共享的数据区域，增加核数的同时会增加等锁时间。
>2. 任务写一个共享区域是，会打破cache的连续性。
>3. 任务写共享内存区域时，增加核数会使cache命中率降低，颠簸严重。
>4. 任务们有可能竞争硬件资源例如内部核通信或DRAM接口，其他的核等资源而不计算。
>5. 有可能没那么多任务来给那么多核做，多增加核数多出不工作的核。

很多症状表明延迟都与cache缺失有关。每个core有一个cache时，当一个
core想要写缓存别的core已缓存的内容，则要将别的core里的缓存信息无效掉。类似等等。从片外读数据需要很长的时钟周期，所以共享易变的数据会比预期降低性能。

####多核包处理####
传输扩展：Linux简单地把出硬件队列的包放在当前的核上，对于要来的包，网卡提供一个接口来配置硬件使得在一个特定核上的具体队列里的包满足特定条件，如源IP地址和端口号。这种设置对长链接好而对短的不好。
本文通过修改accpet，使硬件能决定哪个核以及核上哪个线程会来处理即将到来的包。

####模糊计数器####
Linux使用共享的计数器在多核下会成为一个瓶颈，因为很多核会并发地去更新它。
本文解决：提出sloppy counter，建立在每个核都能有某个对象的一些引用，为了这个核可以持有那些线程跑在它上面。一个sloppy counter代表单个共享中心计数器和一组每个核的引用计数。如果一个核把sloppy counter加V，那么他先尝试在一个引用的计数器上减V，如果它计数器大于V，则成功。否则则要向中心计数器请求引用。一个核要减sloppy counter　V，那么它将释放这些资源，再把自己的计数器加V。
sloppy counter的值是所有core拥有的资源总和。
sloppy counter优点：
>易使用，只需改它自己cache里的数，不用等锁。
>扩展性好，通常没有缓存不命中。
>占内存小，只需O(n)的空间复杂度。

缺点：
>与核的个数成比例地占用空间


####无锁对比####
>如果gengration counter值为0，则回到锁机制；否则记住现在的值
>把dentry的域拷贝到本地。如果后来得到的计数器的值不同于记录的值，则回到有锁机制。
>比较拷贝的域和变量，如果符合，则加reference计数器的值，返回dentry，如果reference　count的值是０，回到有锁机制。

无锁机制增强了可扩展性，因为它允许多核并行的查目录。

####单核数据结构####
重构数据结构，因为单纯的消除锁结构还是会因多核共享一个数据结构产生瓶颈。
把一个超级列表分成几个文件给各个核，各个核读写时加锁，加自己的vsfmount表。基本把集中的都转化为per-core的。

####消除错误共享####
把平凡修改的数据单独放到一个cache line中。

####避免不必要的锁####
锁不是很必要时通过删除特殊的情况来删除锁。
通过把super页映射分裂成一个互斥一个映射。
　　
　　
　　



##实验评估##
用到上述７个应用对所做工作进行评估，实验结果显示，MOSBENCH在48核上有很好的扩展性。总结得出，传统的Linux内核与扩展性的需求是可兼容的。本文还有一些不足，例如在实际应用中，I/O可能成为可扩展的瓶颈。


